# Semantic Communication System (SemCom-Project)

## Project Vision & End Goal
You are building a complete **Semantic Communication System** that consists of both encoder and decoder components.  
The system aims to compress natural language into semantic representations (triples) for efficient communication, then reconstruct meaningful information at the receiver end.

**System Architecture Overview:**

```
INPUT TEXT â†’ [ENCODER: Triple Extraction] â†’ SEMANTIC TRIPLES â†’ [DECODER: Text Reconstruction] â†’ OUTPUT TEXT
```

- **Current Focus:** Working on the **ENCODER** part (triple extraction stage) to convert natural language sentences into structured semantic triples.

---

## Project Structure & File Organization

**Local Laptop Directory Structure:**

```
SEMANTIC COMMS/
â”œâ”€ .vscode/
â”œâ”€ data/
â”‚  â”œâ”€ processed/
â”‚  â””â”€ raw/
â”‚     â””â”€ sample-1.json
â”œâ”€ results/
â”œâ”€ docs/
â”œâ”€ logs/
â”‚  â””â”€ logs.md
â”œâ”€ model_extraction/                    # â† ENCODER STAGE
â”‚  â”œâ”€ data/
â”‚  â”‚  â””â”€ labeled_triples.json
â”‚  â”œâ”€ scripts/
â”‚  â”‚  â””â”€ run_inference.py
â”‚  â””â”€ trained/
â”‚     â””â”€ flan-t5-triple-extraction-final/
â”‚        â”œâ”€ config.json
â”‚        â”œâ”€ generation_config.json
â”‚        â”œâ”€ model.safetensors
â”‚        â”œâ”€ special_tokens_map.json
â”‚        â”œâ”€ spiece.model
â”‚        â”œâ”€ tokenizer_config.json
â”‚        â”œâ”€ tokenizer.json
â”‚        â””â”€ training_args.bin
â”œâ”€ reports/
â”œâ”€ src/
â”‚  â”œâ”€ decoding/                        # â† DECODER STAGE (Future work)
â”‚  â”œâ”€ encoding/                        # â† ENCODER STAGE (Current work)
â”‚  â””â”€ extraction/
â”‚     â””â”€ extracting_entities.py
â”œâ”€ utils/
â”œâ”€ tests/
â”œâ”€ .gitignore
â”œâ”€ README.md
â””â”€ requirements.txt
```

---

## Complete System Workflow

### Stage 1: ENCODER (Current Work - Triple Extraction)
- **Input:** Natural language sentences  
- **Process:** Extract semantic triples using fine-tuned Flan-T5  
- **Output:** Structured triples (`subject|relation|object`)  
- **Status:** âœ… Model trained, ðŸš¨ Output quality issues  

### Stage 2: DECODER (Future Work - Text Reconstruction)
- **Input:** Semantic triples  
- **Process:** Reconstruct meaningful text from triples  
- **Output:** Natural language sentences  
- **Status:** âŒ Not started yet  

**Integration Goal:**  
- Encoder compresses sentences â†’ triples (bandwidth efficient)  
- Decoder reconstructs triples â†’ sentences (information recovery)  

---

## Current Progress Status

### âœ… ENCODER Stage - Completed
- **Dataset Preparation:** Created `labeled_triples.json` with sentence-to-triple mappings  
- **Environment Setup:** Google Colab with transformers library  
- **Model Selection:** `google/flan-t5-large` for sequence-to-sequence learning  
- **Data Preprocessing:** Tokenized dataset with proper input/label formatting  
- **Training Configuration:** `Seq2SeqTrainingArguments` setup  
- **Model Training:** Successfully completed fine-tuning (5 epochs, ~24 minutes)  
- **Model Saving:** Downloaded to local laptop  
- **Inference Setup:** Created `run_inference.py` script  

### ðŸ”§ ENCODER Stage - Issues
- **Critical Problem:** Model outputs input repetition instead of structured triples  
  - Input: `"this table is green"`  
  - Expected: `"table|has color|green"`  
  - Actual: `"this table is green"`  
- **Likely Causes:**
  - Training data may have sentences in both text and triples fields  
  - Preprocessing function mapping errors  
  - Label formatting issues during tokenization  

### âŒ DECODER Stage - Not Started
- **Architecture Design:** Need to define decoder model approach  
- **Training Data:** Need triples-to-text datasets  
- **Model Selection:** Choose appropriate model for text generation from triples  
- **Integration:** Connect encoder-decoder pipeline  
---
## Current Critical Issues

### ðŸš¨ Encoder Model Performance
- **Problem:** Fine-tuned model repeating input instead of extracting triples  
- **Likely Causes:**  
  - Training data issues  
  - Preprocessing/label formatting errors  

### ðŸ”§ Technical Challenges
- **Generation Parameters:** Flan-T5 doesnâ€™t support sampling flags (`top_p`, `top_k`)  
- **Data Validation:** Need to verify training data quality  
- **Evaluation Metrics:** No automated assessment of triple extraction quality  

---

## Immediate Next Steps (Encoder Focus)

### Priority 1 - Fix Current Model
- ðŸ” Inspect Training Data: Verify `labeled_triples.json` format  
- ðŸ”§ Data Correction: Fix malformed triples  
- ðŸ”„ Retrain Model: If data issues found  
- ðŸ§ª Validation: Test with known good inputs  

### Priority 2 - Encoder Completion
- ðŸ“Š Evaluation Pipeline: Implement triple extraction metrics  
- ðŸ“ Documentation: Complete encoder usage guide  
- ðŸ”§ Optimization: Fine-tune generation parameters  
- ðŸ§ª Testing: Comprehensive evaluation suite  

---

## Future Roadmap (Decoder Development)

### Stage 2 Planning
- **Architecture Research:** Survey triple-to-text generation approaches  
- **Data Preparation:** Create/find triples-to-sentence datasets  
- **Model Selection:** Choose decoder architecture (T5, GPT variants, etc.)  
- **Integration Design:** Plan encoder-decoder pipeline  

### System Integration
- **End-to-End Pipeline:** Connect encoder â†’ decoder  
- **Performance Metrics:** Semantic fidelity, compression ratio  
- **Optimization:** Joint training or individual component tuning  
- **Deployment:** Production-ready semantic communication system  

---
### Development Environment
```
transformers>=4.0
torch>=1.9
datasets
numpy
pandas
```

### Hardware
- **Training:** Google Colab (free tier)  
- **Development:** Local Windows laptop  
- **Inference:** CPU/GPU automatic detection  

